# Telegram bot for Vercel that interacts with the OpenAI API

## How to use the OpenAI API

At the moment this is used to study and test the OpenAI API. [OpenAPI documentation](https://openai.com/api/).
1. The OpenAI API processes information in 2020, so the AI may not be aware of current events. We will use the news API to report.

2. The bot does not respond to past messages, the data is processed on the server side. To make the bot remember past messages and respond to them, you will need to store the messages in a database or some other data structure that your bot can access. One approach is to use a key-value store like Redis or Memcached to store past messages. You can use the user's ID as the key and the message as the value.

3. The bot only responds to text messages.

```javascript
const configuration = new Configuration({
    apiKey: process.env.API_KEY,
    timeZone: 'Europe/Kyiv' //Set up OpenAI API with your timezone
});
const openai = new OpenAIApi(configuration);

bot.on('text', async (msg) => {
  if (msg.text.startsWith('/')) return;
  try {
    bot.sendAction(msg.chat.id, 'typing')

    // Retrieve current news headlines
    const newsResponse = await axios.get(`https://newsapi.org/v2/top-headlines?country=us&apiKey=${process.env.NEWS_API}`)
    const headlines = newsResponse.data.articles.map(article => article.title)

    // Get current date
    const today = new Date();
    const year = today.getFullYear();
    const month = today.toLocaleString('default', { month: 'long' });
    const day = today.getDate();

    const response = await openai.createCompletion({
        model: "text-davinci-003",
        prompt: `–°–µ–≥–æ–¥–Ω—è ${day} ${month} ${year}. –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:\n${headlines.join('\n')}\n\n${msg.text}`,
        temperature: 0.3,
        max_tokens: 800,
        top_p: 0.7,
        frequency_penalty: 0,
        presence_penalty: 0,
    })
    // Send response back to user
    await msg.reply.text(response.data.choices[0].text)
  } catch (error) {
    console.error(error)
    await msg.reply.text(`–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫üò±\n –û—à–∏–±–∫–∏ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞`)
  }
})
```

## Using settings for openai

1. model - Parameter specifies which pre-trained language model to use for text generation. The model determines the complexity, quality, and characteristics of the generated text.

2. prompt - Your message, use as many leading questions as possible for a more accurate answer.

3. temperature - In the context of the OpenAI API, the temperature parameter controls the "creativity" or randomness of the text generated by the model. Specifically, temperature determines the standard deviation of the logits of the model's last hidden state before generating the text. A higher temperature value will result in more random and diverse responses, while a lower temperature value will result in more conservative and predictable responses.

4. max_tokens - Used to specify the maximum number of tokens (words or symbols) that OpenAI's GPT model is allowed to generate in its response to the given prompt. The actual number of tokens in the response may be less than the specified maximum, depending on the model's prediction for the given prompt.

5. top_p - In OpenAI's GPT models, top_p (also known as nucleus sampling or probabilistic sampling) is a parameter used during text generation that sets a cumulative probability threshold for selecting the next word in the generated text.

6. frequency_penalty - Used to discourage the model from repeating the same words or phrases in its generated text. This can be useful to avoid repetitive or formulaic language in the model's output.

7. presence_penalty - Used to control how much the generated text should "avoid repeating the same context" as the input prompt.

## How to use for your project

You need to get 3 keys to work with this bot.

1. [OpenAI API](https://openai.com/)

2. [Telegram token](https://telegram.me/BotFather)

3. [New API](https://newsapi.org/) - get information about the latest events

Click the Deploy with Vercel button.

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FPonomareVlad%2FTeleVercelBot&env=TELEGRAM_BOT_TOKEN&env=API_KEY&env=NEWS_API&repo-name=telegram-bot)

Change the project configurations (Variables). 

```javascript
TELEGRAM_BOT_TOKEN = 'your Telegram bot token'
API_KEY = 'your OpenAI API key'
NEWS_API = 'your news API key token'
```

Create a new project on GitHub and deploy it to Vercel. Once created, go to Deployment to your project and click "Set WebHook Url".

## Run locally

You need to create a .env file in the root of the project and add your keys to it. The library [dontev](https://www.npmjs.com/package/dotenv) is used for encryption.

```javascript
TELEGRAM_BOT_TOKEN = 'your Telegram bot token'
API_KEY = 'your OpenAI API key'
NEWS_API = 'your news API key token'
```

Install all dependencies.

```bash
npm i
```

Check if [ngrok](https://ngrok.com/) is installed in your project.

```bash
ngrok -v
```

Install your ngrock authtoken. You can get the token in your personal cabinet.

```bash
ngrok authtoken <you-token>
```

Run localhost in the terminal.

```bash
npm run dev
```

Run ngrok in a new terminal after starting localhost (ngrok http 3000). Where 3000 is your local server port.

```bash
npm run start or ngrok http 3000
```

Example package.json

```javascript
  "scripts": {
    "dev": "vc dev",
    "start": "ngrok http 3000"
  },
```

Open your unique ngrok link and install webhook (click "Set WebHook Url"). Now you can make some changes in [bot.mjs](bot.mjs). After deploying to a production run, do not forget to install a new webhook in Vercel.

[Documentation for TeleBot](https://github.com/mullwar/telebot)

## Template structure:

- [api/telegram.mjs](api/telegram.mjs) ‚Äî Endpoint function for WebHooks
- [api/setWebhook.mjs](api/setWebhook.mjs) ‚Äî Function for setting WebHook URL

###### P.S. Don't forget to remove or restrict [api/setWebhook.mjs](api/setWebhook.mjs) function before going to production